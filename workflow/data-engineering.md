# Data Engineering Workflow

## Overview
Comprehensive data engineering workflow for building, optimizing, and maintaining data pipelines with quality assurance. Covers ETL/ELT processes, data quality, and pipeline orchestration.

## Primary Data Pipeline Workflow

### 1. Requirements Analysis & Design
**juvenile-requirements-analyst**: Analyze business requirements
- Understand data needs and use cases
- Define transformation logic
- Specify quality requirements
- Document SLAs and constraints

**consolidated-fullstack-data-engineer**: Design and implement end-to-end pipeline architecture
- Select ETL vs ELT approach with technology stack evaluation
- Design error handling and scalability strategies
- Assess source data quality across all dimensions
- Profile data and identify quality issues and anomalies
- Generate comprehensive quality baseline report

### 2. Unified Pipeline Implementation & Quality Control
**consolidated-fullstack-data-engineer**: Build integrated data pipeline with quality assurance
- Implement extraction, transformation, and loading logic
- Configure orchestration and scheduling with error handling
- Map data flow paths and document transformations
- Define quality rules and validation checkpoints
- Implement anomaly detection with monitoring dashboards
- Create lineage documentation and dependency tracking

### 3. Testing & Validation
**consolidated-fullstack-data-engineer**: Comprehensive pipeline testing
- Unit test transformations with automated test generation
- Integration test pipeline with comprehensive validation
- Validate data accuracy across all processing stages
- Test error scenarios and recovery mechanisms

### 4. Performance & Cost Optimization
**juvenile-performance-cost-optimizer**: Optimize pipeline performance with cost efficiency
- Profile pipeline execution with resource cost analysis
- Identify bottlenecks with cost-performance tradeoffs
- Optimize data processing for throughput and cost efficiency
- Implement parallel processing and query optimization
- Configure caching and tune batch sizes for optimal cost

## Specialized Data Workflows

### Real-Time Streaming Pipeline
1. **consolidated-fullstack-data-engineer**: Design streaming architecture with quality monitoring
2. **juvenile-infrastructure-architect**: Set up streaming infrastructure with orchestration
3. **observability-enhancer**: Monitor stream processing performance
4. **juvenile-performance-cost-optimizer**: Optimize streaming cost and performance

### Data Migration
1. **consolidated-fullstack-data-engineer**: Pre-migration quality assessment and migration pipeline design
2. **juvenile-database-architect**: Schema mapping and optimization
3. **consolidated-fullstack-data-engineer**: Validate migration with comprehensive testing
4. **consolidated-fullstack-data-engineer**: Post-migration verification and quality assurance

### Data Lake/Warehouse Implementation
1. **juvenile-database-architect**: Design schema architecture
2. **consolidated-fullstack-data-engineer**: Build ingestion pipelines with quality framework
3. **juvenile-infrastructure-architect**: Set up infrastructure with monitoring
4. **juvenile-documentation-curator**: Document architecture and processes

### ML Data Pipeline
1. **consolidated-fullstack-data-engineer**: Build feature engineering pipeline with quality validation
2. **juvenile-ml-ops-engineer**: Integrate with ML systems and infrastructure
3. **juvenile-performance-cost-optimizer**: Optimize feature generation for cost and performance

## Data Quality Management

### Continuous Quality Monitoring
1. **consolidated-fullstack-data-engineer**: Define quality metrics and implement quality checks
2. **observability-enhancer**: Real-time quality monitoring across pipelines
3. **juvenile-incident-responder**: Handle quality incidents and recovery

### Root Cause Analysis
1. **consolidated-fullstack-data-engineer**: Detect quality degradation and trace issues through pipeline
2. **consolidated-fullstack-data-engineer**: Investigate root causes with systematic debugging
3. **consolidated-fullstack-data-engineer**: Implement fixes with comprehensive testing

### Data Governance
1. **consolidated-fullstack-data-engineer**: Define quality standards and data policies
2. **juvenile-documentation-curator**: Document data governance framework
3. **juvenile-security-compliance-guardian**: Ensure privacy and security compliance

## Pipeline Operations

### CI/CD for Data Pipelines
1. **juvenile-infrastructure-architect**: Set up CI/CD pipelines with automated deployment
2. **consolidated-fullstack-data-engineer**: Review pipeline code with comprehensive testing
3. **consolidated-fullstack-data-engineer**: Deploy pipelines with quality assurance

### Monitoring & Alerting
1. **observability-enhancer**: Set up comprehensive monitoring
2. **juvenile-infrastructure-architect**: Configure alerts and monitoring infrastructure
3. **juvenile-incident-responder**: Incident response and recovery
4. **juvenile-log-analyzer**: Analyze pipeline logs and patterns

### Cost Optimization
1. **juvenile-performance-cost-optimizer**: Analyze pipeline costs with performance tradeoffs
2. **consolidated-fullstack-data-engineer**: Optimize resource usage and processing efficiency
3. **juvenile-infrastructure-architect**: Right-size infrastructure for cost efficiency

## Key Data Engineering Metrics
- Pipeline execution time
- Data processing throughput
- Data quality scores (6 dimensions)
- Pipeline failure rate
- Data freshness/latency
- Resource utilization
- Cost per GB processed
- Schema drift incidents

## Success Criteria
- Pipelines meet SLA requirements
- Data quality scores >95%
- Automated monitoring in place
- Error recovery mechanisms functional
- Comprehensive documentation
- Cost-optimized operations
- Scalable architecture